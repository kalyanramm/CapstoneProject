{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalyan Manda\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3146: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# all accepted loans from lendingclub\n",
    "all_accept_df = pd.read_csv('./archive/accepted_2007_to_2018q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fully Paid', 'Current', 'Charged Off', 'In Grace Period',\n",
       "       'Late (31-120 days)', 'Late (16-30 days)', 'Default', 'Unknown',\n",
       "       'Does not meet the credit policy. Status:Fully Paid',\n",
       "       'Does not meet the credit policy. Status:Charged Off'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop invalid loan_status to 'Unknown'\n",
    "\n",
    "all_accept_df.loc[pd.isnull(all_accept_df['loan_status']),'loan_status'] = 'Unknown'\n",
    "\n",
    "all_accept_df.loan_status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e98628443b49>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loan_status[(loan_status=='Fully Paid') | (loan_status=='Current') |\n",
      "<ipython-input-9-e98628443b49>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  loan_status[loan_status != 1] = 0\n"
     ]
    }
   ],
   "source": [
    "loan_status = all_accept_df['loan_status']\n",
    "\n",
    "loan_status[(loan_status=='Fully Paid') | (loan_status=='Current') |\n",
    "            (loan_status=='Does not meet the credit policy. Status:Fully Paid') |\n",
    "            (loan_status=='In Grace Period')] = 1\n",
    "\n",
    "loan_status[loan_status != 1] = 0\n",
    "\n",
    "all_accept_df['loan_status'] = pd.to_numeric(loan_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertDateTimeToOrdinal(d):\n",
    "    if d is pd.NaT:\n",
    "        return 0\n",
    "    else:\n",
    "        return d.toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to datetime (toordinal)\n",
    "\n",
    "dateCols = pd.Series(\n",
    "    ['issue_d',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_date',\n",
    "    'hardship_start_date',\n",
    "    'hardship_end_date',\n",
    "    'payment_plan_start_date',\n",
    "    'sec_app_earliest_cr_line'])\n",
    "\n",
    "for col in dateCols:    \n",
    "    all_accept_df[col] = pd.to_datetime(all_accept_df[col]).apply(ConvertDateTimeToOrdinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns to be ignore for now\n",
    "\n",
    "all_accept_df = all_accept_df.drop([\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'emp_length',\n",
    "    'home_ownership',\n",
    "    'url',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'initial_list_status',\n",
    "    'verification_status_joint',  \n",
    "    'hardship_type',\n",
    "    'hardship_reason',\n",
    "    'disbursement_method'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column to numeric\n",
    "\n",
    "all_accept_df['term'] = pd.to_numeric(all_accept_df['term'].apply(lambda x: re.findall(r'\\d+', str(x))).str[0])\n",
    "all_accept_df['deferral_term'] = pd.to_numeric(all_accept_df['deferral_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify\n",
    "\n",
    "colsToBeDummified = pd.Series(\n",
    "    ['grade',\n",
    "    'sub_grade',\n",
    "    'verification_status',\n",
    "    'purpose',\n",
    "    'pymnt_plan',\n",
    "    'application_type',\n",
    "    'hardship_flag',\n",
    "    'hardship_status',\n",
    "    'hardship_loan_status',\n",
    "    'debt_settlement_flag',\n",
    "    'settlement_status'])\n",
    "\n",
    "for col in colsToBeDummified:    \n",
    "    all_accept_df = all_accept_df.join(pd.get_dummies(all_accept_df[col], drop_first=True, prefix=col))\n",
    "    all_accept_df = all_accept_df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle na\n",
    "\n",
    "all_accept_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_accept_train_df, all_accept_test_df, all_accept_train_target, all_accept_test_target = \\\n",
    "            train_test_split(all_accept_df, all_accept_df['loan_status'], test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-9b912882cf0c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_accept_train_df[col] = pd.to_numeric(all_accept_train_df[col])\n",
      "<ipython-input-17-9b912882cf0c>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_accept_test_df[col] = pd.to_numeric(all_accept_test_df[col])\n"
     ]
    }
   ],
   "source": [
    "# make sure all columns are numeric\n",
    "\n",
    "for col in all_accept_train_df.columns:\n",
    "    all_accept_train_df[col] = pd.to_numeric(all_accept_train_df[col])\n",
    "    all_accept_test_df[col] = pd.to_numeric(all_accept_test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1808560 entries, 732625 to 1873416\n",
      "Columns: 192 entries, loan_amnt to settlement_status_COMPLETE\n",
      "dtypes: float64(113), int64(12), uint8(67)\n",
      "memory usage: 1.8 GB\n"
     ]
    }
   ],
   "source": [
    "all_accept_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 452141 entries, 71127 to 717237\n",
      "Columns: 192 entries, loan_amnt to settlement_status_COMPLETE\n",
      "dtypes: float64(113), int64(12), uint8(67)\n",
      "memory usage: 463.5 MB\n"
     ]
    }
   ],
   "source": [
    "all_accept_test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term',\n",
    "    'int_rate', 'grade_B', 'grade_C',\n",
    "    'grade_D', 'grade_E', 'grade_F', 'grade_G',\n",
    "    'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5', \n",
    "    'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5',\n",
    "    'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4',\n",
    "    'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3',\n",
    "    'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2',\n",
    "    'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1',\n",
    "    'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5',\n",
    "    'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4',\n",
    "    'sub_grade_G5', 'sub_grade_B1','purpose_credit_card', 'purpose_debt_consolidation',\n",
    "    'purpose_home_improvement', 'purpose_medical', 'purpose_other', \n",
    "    'purpose_small_business', 'annual_inc',\n",
    "    'issue_d', 'dti', 'delinq_2yrs', 'fico_range_low', 'fico_range_high',\n",
    "    'open_acc', 'total_acc', 'total_pymnt',\n",
    "    'total_pymnt_inv', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "    'annual_inc_joint', 'dti_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',\n",
    "    'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',\n",
    "    'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = all_accept_train_df.columns.isin(features)\n",
    "\n",
    "kBestColumns = all_accept_train_df.columns[mask]\n",
    "otherColumnsToBeCombined = all_accept_train_df.columns[~mask]\n",
    "\n",
    "len(kBestColumns) + len(otherColumnsToBeCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - PCA on non-kBest columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Modify train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808560, 21)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_train_otherColsDf = pd.DataFrame(all_accept_train_df[otherColumnsToBeCombined])\n",
    "\n",
    "data_rescaled = scaler.fit_transform(all_accept_train_otherColsDf)\n",
    "\n",
    "pca.fit(data_rescaled)\n",
    "\n",
    "reduced = pca.transform(data_rescaled)\n",
    "reduced = pd.DataFrame(reduced)\n",
    "\n",
    "reduced.fillna(0, inplace=True)\n",
    "\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808560, 75)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_accept_train_df[kBestColumns]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1808560, 96)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBestColsDf = pd.DataFrame(all_accept_train_df[kBestColumns])\n",
    "\n",
    "all_accept_train_df = \\\n",
    "    pd.concat([kBestColsDf, reduced.reindex(kBestColsDf.index)], axis=1)\n",
    "\n",
    "all_accept_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Modify test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452141, 21)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_test_otherColsDf = pd.DataFrame(all_accept_test_df[otherColumnsToBeCombined])\n",
    "\n",
    "reduced = pca.transform(all_accept_test_otherColsDf)\n",
    "reduced = pd.DataFrame(reduced)\n",
    "\n",
    "reduced.fillna(0, inplace=True)\n",
    "\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452141, 96)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBestColsDf = pd.DataFrame(all_accept_test_df[kBestColumns])\n",
    "\n",
    "all_accept_test_df = \\\n",
    "    pd.concat([kBestColsDf, reduced.reindex(kBestColsDf.index)], axis=1)\n",
    "\n",
    "all_accept_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1808560"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_train_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrink data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_range_train = range(1, all_accept_train_df.shape[0])\n",
    "#n_range_train = range(1, 20000)\n",
    "\n",
    "all_accept_train_df_r = pd.DataFrame(all_accept_train_df.iloc[n_range_train])\n",
    "all_accept_train_target_r = all_accept_train_target.iloc[n_range_train] \n",
    "\n",
    "all_accept_train_df_r.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_range_test = range(1, all_accept_test_df.shape[0])\n",
    "#n_range_test = range(1, 2000)\n",
    "\n",
    "all_accept_test_df_r = pd.DataFrame(all_accept_test_df.iloc[n_range_test])\n",
    "all_accept_test_target_r = all_accept_test_target.iloc[n_range_test] \n",
    "\n",
    "all_accept_test_df_r.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 100\n",
    "n_jobs = 6\n",
    "#n_jobs = 2\n",
    "cv = 5\n",
    "accuracy = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "\n",
    "grid_para_tree = [{\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_leaf\": range(1, 10),\n",
    "    \"min_samples_split\": np.linspace(start=2, stop=30, num=15, dtype=int)\n",
    "}]\n",
    "\n",
    "tree_model.set_params(random_state=random_state)\n",
    "\n",
    "grid_search_tree = model_selection.GridSearchCV(tree_model, grid_para_tree,\n",
    "                                                cv=cv, scoring=accuracy,\n",
    "                                                n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14h 3min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=DecisionTreeClassifier(random_state=100), n_jobs=6,\n",
       "             param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                          'min_samples_leaf': range(1, 10),\n",
       "                          'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30])}],\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_search_tree.fit(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9916320120051378"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tree.score(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9868845932675719"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tree.score(all_accept_test_df_r, all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 225034,    4009],\n",
       "       [  11125, 1568391]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_tree.predict(all_accept_train_df_r), all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 55124,   2005],\n",
       "       [  3925, 391086]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_tree.predict(all_accept_test_df_r), all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_c = xgb.XGBClassifier(\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=n_jobs,\n",
    "    seed=random_state\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (5, 7, 10),\n",
    "    'n_estimators': range(100, 150, 200),\n",
    "    'learning_rate': [0.25, 0.2, 0.1]\n",
    "}\n",
    "\n",
    "grid_search_xgb = model_selection.GridSearchCV(\n",
    "    estimator=xgb_c,\n",
    "    param_grid=parameters,\n",
    "    scoring = accuracy,\n",
    "    n_jobs = n_jobs,\n",
    "    cv = cv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kalyan Manda\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:50:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 47min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None, nthread=6,\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, seed=100,\n",
       "                                     subsample=None, tree_method=None,\n",
       "                                     validate_parameters=None, verbosity=None),\n",
       "             n_jobs=6,\n",
       "             param_grid={'learning_rate': [0.25, 0.2, 0.1],\n",
       "                         'max_depth': range(5, 7, 10),\n",
       "                         'n_estimators': range(100, 150, 200)},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_search_xgb.fit(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917414914304703"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgb.score(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9911797230946167"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgb.score(all_accept_test_df_r, all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 221916,     693],\n",
       "       [  14243, 1571707]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_xgb.predict(all_accept_train_df_r), all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 55264,    203],\n",
       "       [  3785, 392888]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_xgb.predict(all_accept_test_df_r), all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import sklearn\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kalyan Manda\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers\\initializers_v1.py:67: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "class One_layer_batch(object):\n",
    "    kernel_initializer = initializers.TruncatedNormal(mean=0.0,stddev=0.1)\n",
    "    bias_initializer   = initializers.Constant(0.1)    \n",
    "    def __init__(self, size_hidden=2):\n",
    "        self.__w1 = None\n",
    "        self.__b1 = None\n",
    "        self.__w2 = None\n",
    "        self.__b1 = None\n",
    "        self.__size_hidden = size_hidden\n",
    "        \n",
    "    def __initialize_model(self, x_train, y_train, rate):\n",
    "        num_observations= x_train.shape[0]\n",
    "        num_features   = x_train.shape[1]\n",
    "        num_labels     = y_train.shape[1]\n",
    "        size_hidden    = self.__size_hidden\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "        \n",
    "        d_layer1 = Dense(input_shape=(num_features,), units=size_hidden, activation=tf.nn.sigmoid, kernel_initializer = self.kernel_initializer, bias_initializer= self.bias_initializer)\n",
    "        hidden   = d_layer1(x)\n",
    "        \n",
    "        hidden_drop = tf.nn.dropout(hidden, rate=rate)\n",
    "        \n",
    "        d_layer2 = Dense(input_shape=(size_hidden,), units=num_labels, kernel_initializer = self.kernel_initializer, bias_initializer= self.bias_initializer)\n",
    "        y_lin   = d_layer2(hidden_drop)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_lin, labels=y))\n",
    "        \n",
    "        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        return train_step, x, y, d_layer1.weights[0], d_layer1.weights[1], d_layer2.weights[0], d_layer2.weights[1]\n",
    "        \n",
    "    def fit(self, x_train, y_train, rate = 0.25, batch_size = 100, steps = 10000):\n",
    "        train_step, x, y, w_1, b_1, w_2, b_2 = self.__initialize_model(x_train, y_train, rate)\n",
    "        \n",
    "        num_observations = x_train.shape[0]\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for i in range(steps):\n",
    "            locs    = sklearn.utils.random.sample_without_replacement(num_observations, batch_size)\n",
    "            x_batch = x_train.values[locs,:]\n",
    "            y_batch = y_train[locs,:]\n",
    "            train_step.run(feed_dict={x: x_batch, y: y_batch})\n",
    "            \n",
    "        self.__w1 = w_1.eval()\n",
    "        self.__b1 = b_1.eval()\n",
    "        self.__w2 = w_2.eval()\n",
    "        self.__b2 = b_2.eval()\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return scipy.special.expit(x)\n",
    "        \n",
    "    def predict(self, x_train):\n",
    "        w1 = self.__w1\n",
    "        b1 = self.__b1\n",
    "        w2 = self.__w2\n",
    "        b2 = self.__b2\n",
    "        y_lin = np.dot(self.__sigmoid(np.dot(x_train, w1)+b1), w2)+b2\n",
    "        return tf.argmax(y_lin, 1).eval()        \n",
    "    \n",
    "    @property\n",
    "    def w1(self):\n",
    "        return self.__w1\n",
    "    \n",
    "    @property\n",
    "    def b1(self):\n",
    "        return self.__b1\n",
    "    \n",
    "    @property\n",
    "    def w2(self):\n",
    "        return self.__w2\n",
    "    \n",
    "    @property\n",
    "    def b2(self):\n",
    "        return self.__b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.8694214565297566\n",
      "Test Accuracy:  0.8694010704649002\n",
      "Time elapsed:  32186.697366952896\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "all_accept_train_target_r_dum = pd.get_dummies(pd.Series(all_accept_train_target_r)).values\n",
    "all_accept_test_target_r_dum = pd.get_dummies(pd.Series(all_accept_test_target_r)).values\n",
    "\n",
    "nn = One_layer_batch(size_hidden=40)\n",
    "nn.fit(all_accept_train_df_r, all_accept_train_target_r_dum, batch_size = 100, steps = 50000)\n",
    "\n",
    "predict = nn.predict(all_accept_train_df_r)\n",
    "print ('Training Accuracy: ', np.mean(predict == np.argmax(all_accept_train_target_r_dum, 1)))\n",
    "\n",
    "predict = nn.predict(all_accept_test_df_r)\n",
    "print('Test Accuracy: ', np.mean(predict == np.argmax(all_accept_test_target_r_dum, 1)))\n",
    "\n",
    "print('Time elapsed: ', time.time()-start)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
