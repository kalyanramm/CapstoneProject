{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kmanda/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (0,19,49,59,118,129,130,131,134,135,136,139,145,146,147) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# all accepted loans from lendingclub\n",
    "all_accept_df = pd.read_csv('./archive/accepted_2007_to_2018q4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to numeric\n",
    "\n",
    "all_accept_df['term'] = pd.to_numeric(all_accept_df['term'].apply(lambda x: re.findall(r'\\d+', str(x))).str[0])\n",
    "\n",
    "# convert columns to date\n",
    "\n",
    "all_accept_df['issue_d'] = pd.to_datetime(all_accept_df['issue_d'])\n",
    "all_accept_df['last_pymnt_d'] = pd.to_datetime(all_accept_df['last_pymnt_d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddMonthsToDate(dates, months):\n",
    "    updatedDates = []\n",
    "    \n",
    "    for i in range(0, len(dates)):\n",
    "        toAddMnths = pd.to_numeric(months.iat[i]) - 3\n",
    "        updatedDates.append(dates.iat[i] + pd.DateOffset(months=toAddMnths))\n",
    "        \n",
    "    return updatedDates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1078739\n",
      "1078739\n"
     ]
    }
   ],
   "source": [
    "all_accept_df = all_accept_df[(all_accept_df['loan_status'] == 'Fully Paid') | (all_accept_df['loan_status'] == 'Does not meet the credit policy. Status:Fully Paid')]\n",
    "\n",
    "print(len(all_accept_df['issue_d']))\n",
    "\n",
    "updatedDates = AddMonthsToDate(all_accept_df['issue_d'], all_accept_df['term'])\n",
    "\n",
    "print(len(updatedDates))\n",
    "\n",
    "all_accept_df['loan_prepaid'] = all_accept_df['last_pymnt_d'] < updatedDates\n",
    "all_accept_df = all_accept_df.drop('loan_status', axis=1)\n",
    "\n",
    "#all_accept_df['loan_prepaid'] = ['Prepaid' if x==1 else 'Fully Paid' for x in all_accept_df['loan_prepaid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     805910\n",
       "False    272829\n",
       "Name: loan_prepaid, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_df['loan_prepaid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertDateTimeToOrdinal(d):\n",
    "    if d is pd.NaT:\n",
    "        return 0\n",
    "    else:\n",
    "        return d.toordinal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert columns to datetime (toordinal)\n",
    "\n",
    "dateCols = pd.Series(\n",
    "    ['issue_d',\n",
    "    'earliest_cr_line',\n",
    "    'last_pymnt_d',\n",
    "    'next_pymnt_d',\n",
    "    'last_credit_pull_d',\n",
    "    'debt_settlement_flag_date',\n",
    "    'settlement_date',\n",
    "    'hardship_start_date',\n",
    "    'hardship_end_date',\n",
    "    'payment_plan_start_date',\n",
    "    'sec_app_earliest_cr_line'])\n",
    "\n",
    "for col in dateCols:    \n",
    "    all_accept_df[col] = pd.to_datetime(all_accept_df[col]).apply(ConvertDateTimeToOrdinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns to be ignore for now\n",
    "\n",
    "all_accept_df = all_accept_df.drop([\n",
    "    'id',\n",
    "    'member_id',\n",
    "    'emp_title',\n",
    "    'emp_length',\n",
    "    'home_ownership',\n",
    "    'url',\n",
    "    'desc',\n",
    "    'title',\n",
    "    'zip_code',\n",
    "    'addr_state',\n",
    "    'initial_list_status',\n",
    "    'verification_status_joint',  \n",
    "    'hardship_type',\n",
    "    'hardship_reason',\n",
    "    'disbursement_method'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert column to numeric\n",
    "\n",
    "all_accept_df['term'] = pd.to_numeric(all_accept_df['term'].apply(lambda x: re.findall(r'\\d+', str(x))).str[0])\n",
    "all_accept_df['deferral_term'] = pd.to_numeric(all_accept_df['deferral_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify\n",
    "\n",
    "colsToBeDummified = pd.Series(\n",
    "    ['grade',\n",
    "    'sub_grade',\n",
    "    'verification_status',\n",
    "    'purpose',\n",
    "    'pymnt_plan',\n",
    "    'application_type',\n",
    "    'hardship_flag',\n",
    "    'hardship_status',\n",
    "    'hardship_loan_status',\n",
    "    'debt_settlement_flag',\n",
    "    'settlement_status'])\n",
    "\n",
    "for col in colsToBeDummified:    \n",
    "    all_accept_df = all_accept_df.join(pd.get_dummies(all_accept_df[col], drop_first=True, prefix=col))\n",
    "    all_accept_df = all_accept_df.drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle na\n",
    "\n",
    "all_accept_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "all_accept_train_df, all_accept_test_df, all_accept_train_target, all_accept_test_target = \\\n",
    "            train_test_split(all_accept_df, all_accept_df['loan_prepaid'], test_size=0.2, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kmanda/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/kmanda/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# make sure all columns are numeric\n",
    "\n",
    "for col in all_accept_train_df.columns:\n",
    "    all_accept_train_df[col] = pd.to_numeric(all_accept_train_df[col])\n",
    "    all_accept_test_df[col] = pd.to_numeric(all_accept_test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 862991 entries, 735086 to 297867\n",
      "Columns: 188 entries, loan_amnt to settlement_status_BROKEN\n",
      "dtypes: bool(1), float64(112), int64(12), uint8(63)\n",
      "memory usage: 875.7 MB\n"
     ]
    }
   ],
   "source": [
    "all_accept_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 215748 entries, 187093 to 991606\n",
      "Columns: 188 entries, loan_amnt to settlement_status_BROKEN\n",
      "dtypes: bool(1), float64(112), int64(12), uint8(63)\n",
      "memory usage: 218.9 MB\n"
     ]
    }
   ],
   "source": [
    "all_accept_test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term',\n",
    "    'int_rate', 'grade_B', 'grade_C',\n",
    "    'grade_D', 'grade_E', 'grade_F', 'grade_G',\n",
    "    'sub_grade_A2', 'sub_grade_A3', 'sub_grade_A4', 'sub_grade_A5', \n",
    "    'sub_grade_B2', 'sub_grade_B3', 'sub_grade_B4', 'sub_grade_B5',\n",
    "    'sub_grade_C1', 'sub_grade_C2', 'sub_grade_C3', 'sub_grade_C4',\n",
    "    'sub_grade_C5', 'sub_grade_D1', 'sub_grade_D2', 'sub_grade_D3',\n",
    "    'sub_grade_D4', 'sub_grade_D5', 'sub_grade_E1', 'sub_grade_E2',\n",
    "    'sub_grade_E3', 'sub_grade_E4', 'sub_grade_E5', 'sub_grade_F1',\n",
    "    'sub_grade_F2', 'sub_grade_F3', 'sub_grade_F4', 'sub_grade_F5',\n",
    "    'sub_grade_G1', 'sub_grade_G2', 'sub_grade_G3', 'sub_grade_G4',\n",
    "    'sub_grade_G5', 'sub_grade_B1','purpose_credit_card', 'purpose_debt_consolidation',\n",
    "    'purpose_home_improvement', 'purpose_medical', 'purpose_other', \n",
    "    'purpose_small_business', 'annual_inc',\n",
    "    'issue_d', 'dti', 'delinq_2yrs', 'fico_range_low', 'fico_range_high',\n",
    "    'open_acc', 'total_acc', 'total_pymnt',\n",
    "    'total_pymnt_inv', 'last_pymnt_d', 'last_pymnt_amnt',\n",
    "    'annual_inc_joint', 'dti_joint', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal',\n",
    "    'tot_hi_cred_lim', 'total_bal_ex_mort', 'total_bc_limit', 'total_il_high_credit_limit',\n",
    "    'revol_bal_joint', 'sec_app_fico_range_low', 'sec_app_fico_range_high',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = all_accept_train_df.columns.isin(features)\n",
    "\n",
    "kBestColumns = all_accept_train_df.columns[mask]\n",
    "otherColumnsToBeCombined = all_accept_train_df.columns[~mask]\n",
    "\n",
    "len(kBestColumns) + len(otherColumnsToBeCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - PCA on non-kBest columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Modify train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(862991, 20)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_train_otherColsDf = pd.DataFrame(all_accept_train_df[otherColumnsToBeCombined])\n",
    "\n",
    "data_rescaled = scaler.fit_transform(all_accept_train_otherColsDf)\n",
    "\n",
    "pca.fit(data_rescaled)\n",
    "\n",
    "reduced = pca.transform(data_rescaled)\n",
    "reduced = pd.DataFrame(reduced)\n",
    "\n",
    "reduced.fillna(0, inplace=True)\n",
    "\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(862991, 75)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_accept_train_df[kBestColumns]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(862991, 95)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBestColsDf = pd.DataFrame(all_accept_train_df[kBestColumns])\n",
    "\n",
    "all_accept_train_df = \\\n",
    "    pd.concat([kBestColsDf, reduced.reindex(kBestColsDf.index)], axis=1)\n",
    "\n",
    "all_accept_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Modify test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215748, 20)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_accept_test_otherColsDf = pd.DataFrame(all_accept_test_df[otherColumnsToBeCombined])\n",
    "\n",
    "reduced = pca.transform(all_accept_test_otherColsDf)\n",
    "reduced = pd.DataFrame(reduced)\n",
    "\n",
    "reduced.fillna(0, inplace=True)\n",
    "\n",
    "reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(215748, 95)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kBestColsDf = pd.DataFrame(all_accept_test_df[kBestColumns])\n",
    "\n",
    "all_accept_test_df = \\\n",
    "    pd.concat([kBestColsDf, reduced.reindex(kBestColsDf.index)], axis=1)\n",
    "\n",
    "all_accept_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrink data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_range_train = range(1, all_accept_train_df.shape[0])\n",
    "n_range_train = range(1, 20000)\n",
    "\n",
    "all_accept_train_df_r = pd.DataFrame(all_accept_train_df.iloc[n_range_train])\n",
    "all_accept_train_target_r = all_accept_train_target.iloc[n_range_train] \n",
    "\n",
    "all_accept_train_df_r.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_range_test = range(1, all_accept_test_df.shape[0])\n",
    "n_range_test = range(1, 2000)\n",
    "\n",
    "all_accept_test_df_r = pd.DataFrame(all_accept_test_df.iloc[n_range_test])\n",
    "all_accept_test_target_r = all_accept_test_target.iloc[n_range_test] \n",
    "\n",
    "all_accept_test_df_r.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 100\n",
    "#n_jobs = 6\n",
    "n_jobs = 2\n",
    "cv = 5\n",
    "accuracy = 'accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tree_model = tree.DecisionTreeClassifier()\n",
    "\n",
    "grid_para_tree = [{\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_leaf\": range(1, 10),\n",
    "    \"min_samples_split\": np.linspace(start=2, stop=30, num=15, dtype=int)\n",
    "}]\n",
    "\n",
    "tree_model.set_params(random_state=random_state)\n",
    "\n",
    "grid_search_tree = model_selection.GridSearchCV(tree_model, grid_para_tree,\n",
    "                                                cv=cv, scoring=accuracy,\n",
    "                                                n_jobs=n_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.2 s, sys: 1.48 s, total: 6.69 s\n",
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features=None,\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              presort='deprecated',\n",
       "                                              random_state=100,\n",
       "                                              splitter='best'),\n",
       "             iid='deprecated', n_jobs=2,\n",
       "             param_grid=[{'criterion': ['gini', 'entropy'],\n",
       "                          'min_samples_leaf': range(1, 10),\n",
       "                          'min_samples_split': array([ 2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30])}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_search_tree.fit(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9897994899744987"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tree.score(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874937468734367"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tree.score(all_accept_test_df_r, all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4953,   120],\n",
       "       [   84, 14842]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_tree.predict(all_accept_train_df_r), all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 491,   18],\n",
       "       [   7, 1483]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_tree.predict(all_accept_test_df_r), all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "xgb_c = xgb.XGBClassifier(\n",
    "    objective= 'binary:logistic',\n",
    "    nthread=n_jobs,\n",
    "    seed=random_state\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'max_depth': range (5, 7, 10),\n",
    "    'n_estimators': range(100, 150, 200),\n",
    "    'learning_rate': [0.25, 0.2, 0.1]\n",
    "}\n",
    "\n",
    "grid_search_xgb = model_selection.GridSearchCV(\n",
    "    estimator=xgb_c,\n",
    "    param_grid=parameters,\n",
    "    scoring = accuracy,\n",
    "    n_jobs = n_jobs,\n",
    "    cv = cv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.9 s, sys: 88.1 ms, total: 13 s\n",
      "Wall time: 52.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=2, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=100, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=2,\n",
       "             param_grid={'learning_rate': [0.25, 0.2, 0.1],\n",
       "                         'max_depth': range(5, 7, 10),\n",
       "                         'n_estimators': range(100, 150, 200)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time grid_search_xgb.fit(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgb.score(all_accept_train_df_r, all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9909954977488744"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_xgb.score(all_accept_test_df_r, all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5037,     0],\n",
       "       [    0, 14962]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_xgb.predict(all_accept_train_df_r), all_accept_train_target_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 492,   12],\n",
       "       [   6, 1489]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(grid_search_xgb.predict(all_accept_test_df_r), all_accept_test_target_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import sklearn\n",
    "import time\n",
    "import scipy\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras import initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class One_layer_batch(object):\n",
    "    kernel_initializer = initializers.TruncatedNormal(mean=0.0,stddev=0.1)\n",
    "    bias_initializer   = initializers.Constant(0.1)    \n",
    "    def __init__(self, size_hidden=2):\n",
    "        self.__w1 = None\n",
    "        self.__b1 = None\n",
    "        self.__w2 = None\n",
    "        self.__b1 = None\n",
    "        self.__size_hidden = size_hidden\n",
    "        \n",
    "    def __initialize_model(self, x_train, y_train, rate):\n",
    "        num_observations= x_train.shape[0]\n",
    "        num_features   = x_train.shape[1]\n",
    "        num_labels     = y_train.shape[1]\n",
    "        size_hidden    = self.__size_hidden\n",
    "        \n",
    "        x = tf.placeholder(tf.float32, shape=[None, num_features])\n",
    "        y = tf.placeholder(tf.float32, shape=[None, num_labels])\n",
    "        \n",
    "        d_layer1 = Dense(input_shape=(num_features,), units=size_hidden, activation=tf.nn.sigmoid, kernel_initializer = self.kernel_initializer, bias_initializer= self.bias_initializer)\n",
    "        hidden   = d_layer1(x)\n",
    "        \n",
    "        hidden_drop = tf.nn.dropout(hidden, rate=rate)\n",
    "        \n",
    "        d_layer2 = Dense(input_shape=(size_hidden,), units=num_labels, kernel_initializer = self.kernel_initializer, bias_initializer= self.bias_initializer)\n",
    "        y_lin   = d_layer2(hidden_drop)\n",
    "\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_lin, labels=y))\n",
    "        \n",
    "        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        return train_step, x, y, d_layer1.weights[0], d_layer1.weights[1], d_layer2.weights[0], d_layer2.weights[1]\n",
    "        \n",
    "    def fit(self, x_train, y_train, rate = 0.25, batch_size = 100, steps = 10000):\n",
    "        train_step, x, y, w_1, b_1, w_2, b_2 = self.__initialize_model(x_train, y_train, rate)\n",
    "        \n",
    "        num_observations = x_train.shape[0]\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        for i in range(steps):\n",
    "            locs    = sklearn.utils.random.sample_without_replacement(num_observations, batch_size)\n",
    "            x_batch = x_train.values[locs,:]\n",
    "            y_batch = y_train[locs,:]\n",
    "            train_step.run(feed_dict={x: x_batch, y: y_batch})\n",
    "            \n",
    "        self.__w1 = w_1.eval()\n",
    "        self.__b1 = b_1.eval()\n",
    "        self.__w2 = w_2.eval()\n",
    "        self.__b2 = b_2.eval()\n",
    "    \n",
    "    def __sigmoid(self, x):\n",
    "        return scipy.special.expit(x)\n",
    "        \n",
    "    def predict(self, x_train):\n",
    "        w1 = self.__w1\n",
    "        b1 = self.__b1\n",
    "        w2 = self.__w2\n",
    "        b2 = self.__b2\n",
    "        y_lin = np.dot(self.__sigmoid(np.dot(x_train, w1)+b1), w2)+b2\n",
    "        return tf.argmax(y_lin, 1).eval()        \n",
    "    \n",
    "    @property\n",
    "    def w1(self):\n",
    "        return self.__w1\n",
    "    \n",
    "    @property\n",
    "    def b1(self):\n",
    "        return self.__b1\n",
    "    \n",
    "    @property\n",
    "    def w2(self):\n",
    "        return self.__w2\n",
    "    \n",
    "    @property\n",
    "    def b2(self):\n",
    "        return self.__b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.7481374068703435\n",
      "Test Accuracy:  0.7508754377188595\n",
      "Time elapsed:  233.304438829422\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "all_accept_train_target_r_dum = pd.get_dummies(pd.Series(all_accept_train_target_r)).values\n",
    "all_accept_test_target_r_dum = pd.get_dummies(pd.Series(all_accept_test_target_r)).values\n",
    "\n",
    "nn = One_layer_batch(size_hidden=40)\n",
    "nn.fit(all_accept_train_df_r, all_accept_train_target_r_dum, batch_size = 100, steps = 50000)\n",
    "\n",
    "predict = nn.predict(all_accept_train_df_r)\n",
    "print ('Training Accuracy: ', np.mean(predict == np.argmax(all_accept_train_target_r_dum, 1)))\n",
    "\n",
    "predict = nn.predict(all_accept_test_df_r)\n",
    "print('Test Accuracy: ', np.mean(predict == np.argmax(all_accept_test_target_r_dum, 1)))\n",
    "\n",
    "print('Time elapsed: ', time.time()-start)\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
